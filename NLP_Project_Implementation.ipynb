{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installations"
      ],
      "metadata": {
        "id": "isxZ54nh9yA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install nltk\n",
        "!pip install emoji\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzfQQiK29qNq",
        "outputId": "3b92eabf-41cb-49a6-e08f-36fc2a09a95e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.12.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "Rvta0b1LGINL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import emoji\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "7JwWmCzOGI7Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "QrwFoD7eGX2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize NLTK components\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Clean and preprocess text functions\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)  # Remove hashtags\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
        "    text = text.strip()  # Remove leading/trailing whitespace\n",
        "    return text\n",
        "\n",
        "def correct_text(text):\n",
        "    corrections = {\n",
        "        \"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
        "        \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
        "        \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
        "        \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n",
        "        \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n",
        "        \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n",
        "        \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n",
        "        \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "        \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n",
        "        \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "        \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n",
        "        \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
        "    }\n",
        "    for word, replacement in corrections.items():\n",
        "        text = text.replace(word, replacement)\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return ''\n",
        "    text = clean_text(text)\n",
        "    text = correct_text(text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "def handle_emojis(text):\n",
        "    return emoji.demojize(text)\n",
        "\n",
        "def preprocess_dataframe(df, text_column):\n",
        "    if df[text_column].isnull().sum() > 0:\n",
        "        print(f\"Found {df[text_column].isnull().sum()} NaN values in the {text_column} column. Replacing with empty strings.\")\n",
        "        df[text_column] = df[text_column].fillna('')\n",
        "\n",
        "    df[text_column] = df[text_column].apply(handle_emojis)\n",
        "    df[text_column] = df[text_column].apply(preprocess_text)\n",
        "    return df\n",
        "\n",
        "def preprocess_datasets(train_path, test_path):\n",
        "    train_df = pd.read_csv('/content/train.csv')\n",
        "    test_df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "    train_df = preprocess_dataframe(train_df, 'text')\n",
        "    test_df = preprocess_dataframe(test_df, 'text')\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train = vectorizer.fit_transform(train_df['text']).toarray()\n",
        "    X_test = vectorizer.transform(test_df['text']).toarray()\n",
        "\n",
        "    train_df.to_csv('preprocessed_train.csv', index=False)    #Change index to True if u don't want the file to be downloaded\n",
        "    test_df.to_csv('preprocessed_test.csv', index=False)    #Change index to True if u don't want the file to be downloaded\n",
        "\n",
        "    return X_train, X_test, train_df, test_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjBL3l0iGYNv",
        "outputId": "5989f89c-d357-450c-b482-4a9ce708991a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing for the models"
      ],
      "metadata": {
        "id": "yoMrMYUxJP4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/train.csv'  # Update with actual path to your train.csv\n",
        "test_path = '/content/test.csv'    # Update with actual path to your test.csv\n",
        "\n",
        "X_train, X_test, train_df, test_df = preprocess_datasets(train_path, test_path)\n",
        "\n",
        "print(f\"The shape of the TF-IDF matrix for training data is: {X_train.shape}\")\n",
        "print(f\"The shape of the TF-IDF matrix for test data is: {X_test.shape}\")\n",
        "\n",
        "# Train-test split for evaluation\n",
        "X_train_eval, X_val_eval, y_train_eval, y_val_eval = train_test_split(X_train, train_df['target'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUxaMB0BJQQq",
        "outputId": "bf3ce6c0-e8fb-4e6d-ccf6-a378d0ec467f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the TF-IDF matrix for training data is: (7613, 5000)\n",
            "The shape of the TF-IDF matrix for test data is: (3263, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1: Logistic Regression"
      ],
      "metadata": {
        "id": "R7y2HeNG92nn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhuvYtUU7t8y",
        "outputId": "a386f943-1983-48ce-a59f-6fbbdc3aa6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.89      0.84       874\n",
            "           1       0.83      0.68      0.74       649\n",
            "\n",
            "    accuracy                           0.80      1523\n",
            "   macro avg       0.81      0.79      0.79      1523\n",
            "weighted avg       0.80      0.80      0.80      1523\n",
            "\n",
            "Accuracy: 0.8010505581089954\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_eval, y_train_eval)\n",
        "y_pred_log_reg = log_reg.predict(X_val_eval)\n",
        "print(\"Logistic Regression Performance:\")\n",
        "print(classification_report(y_val_eval, y_pred_log_reg))\n",
        "print(\"Accuracy:\", accuracy_score(y_val_eval, y_pred_log_reg))\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_predictions_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Prepare submission file\n",
        "submission_df_log_reg = pd.DataFrame({'id': test_df['id'], 'target': test_predictions_log_reg})\n",
        "submission_df_log_reg.to_csv('Submission_LogReg.csv', index=False)    #Change index to True if u don't want the file to be downloaded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2: Random Forest"
      ],
      "metadata": {
        "id": "gk4rz52UJkUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_eval, y_train_eval)\n",
        "y_pred_rf = rf.predict(X_val_eval)\n",
        "print(\"Random Forest Performance:\")\n",
        "print(classification_report(y_val_eval, y_pred_rf))\n",
        "print(\"Accuracy:\", accuracy_score(y_val_eval, y_pred_rf))\n",
        "\n",
        "# Prepare test data and make predictions\n",
        "test_predictions_rf = rf.predict(X_test)\n",
        "\n",
        "# Prepare submission file\n",
        "submission_df_rf = pd.DataFrame({'id': test_df['id'], 'target': test_predictions_rf})\n",
        "submission_df_rf.to_csv('Submission_RF.csv', index=False)     #Change index to True if u don't want the file to be downloaded\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CImt4WzRJkg7",
        "outputId": "2a329dfe-42d4-413d-e686-5b2a338fd025"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.82      0.80       874\n",
            "           1       0.74      0.69      0.71       649\n",
            "\n",
            "    accuracy                           0.77      1523\n",
            "   macro avg       0.76      0.76      0.76      1523\n",
            "weighted avg       0.77      0.77      0.76      1523\n",
            "\n",
            "Accuracy: 0.7662508207485227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 3: LSTM"
      ],
      "metadata": {
        "id": "qNkUWD5aJz4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model\n",
        "# Tokenizer and sequence padding for LSTM\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_df['text'])\n",
        "X_seq = tokenizer.texts_to_sequences(train_df['text'])\n",
        "X_pad = pad_sequences(X_seq, maxlen=100)\n",
        "\n",
        "# Train-test split for LSTM\n",
        "X_train_pad, X_val_pad, y_train_pad, y_val_pad = train_test_split(X_pad, train_df['target'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=5000, output_dim=100, input_length=100))\n",
        "lstm_model.add(SpatialDropout1D(0.2))\n",
        "lstm_model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile and train LSTM model\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train_pad, y_train_pad, epochs=5, batch_size=64, validation_data=(X_val_pad, y_val_pad), verbose=2)\n",
        "\n",
        "# Evaluate LSTM model\n",
        "y_pred_lstm = (lstm_model.predict(X_val_pad) > 0.5).astype(\"int32\")\n",
        "print(\"LSTM Model Performance:\")\n",
        "print(classification_report(y_val_pad, y_pred_lstm))\n",
        "print(\"Accuracy:\", accuracy_score(y_val_pad, y_pred_lstm))\n",
        "\n",
        "# Prepare test data and make predictions\n",
        "test_seq = tokenizer.texts_to_sequences(test_df['text'])\n",
        "test_pad = pad_sequences(test_seq, maxlen=100)\n",
        "test_predictions = (lstm_model.predict(test_pad) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Prepare submission file\n",
        "submission_df = pd.DataFrame({'id': test_df['id'], 'target': test_predictions.ravel()})\n",
        "submission_df.to_csv('Submission_LSTM.csv', index=False)      #Change index to True if u don't want the file to be downloaded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-PRq2zSJ0HI",
        "outputId": "73d5d46c-8fb7-41a3-822c-27ce8fd55edf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "96/96 - 39s - loss: 0.5923 - accuracy: 0.6808 - val_loss: 0.4586 - val_accuracy: 0.7991 - 39s/epoch - 401ms/step\n",
            "Epoch 2/5\n",
            "96/96 - 30s - loss: 0.3787 - accuracy: 0.8376 - val_loss: 0.4688 - val_accuracy: 0.7879 - 30s/epoch - 316ms/step\n",
            "Epoch 3/5\n",
            "96/96 - 33s - loss: 0.3008 - accuracy: 0.8762 - val_loss: 0.5136 - val_accuracy: 0.7682 - 33s/epoch - 345ms/step\n",
            "Epoch 4/5\n",
            "96/96 - 32s - loss: 0.2483 - accuracy: 0.9021 - val_loss: 0.6009 - val_accuracy: 0.7656 - 32s/epoch - 338ms/step\n",
            "Epoch 5/5\n",
            "96/96 - 45s - loss: 0.2104 - accuracy: 0.9181 - val_loss: 0.6126 - val_accuracy: 0.7597 - 45s/epoch - 473ms/step\n",
            "48/48 [==============================] - 3s 62ms/step\n",
            "LSTM Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.80      0.79       874\n",
            "           1       0.72      0.70      0.71       649\n",
            "\n",
            "    accuracy                           0.76      1523\n",
            "   macro avg       0.75      0.75      0.75      1523\n",
            "weighted avg       0.76      0.76      0.76      1523\n",
            "\n",
            "Accuracy: 0.7596848325673013\n",
            "102/102 [==============================] - 3s 34ms/step\n"
          ]
        }
      ]
    }
  ]
}